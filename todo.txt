Long-term:
- Use PGD or an open-source verifier to generate more test cases
- When noticed, mark suspicious lines as TODO or DEBUG with a comment
- Reduce the size of the computation graph of loss and znet
- Write a hyperparameter search in order to test different optimizer and different parameters of these optimizers, doing CV on mnist with random epsilons

Todo before production:
- check for TODOs in the source
- remove or review lines marked with a "DEBUG" comment
- make sure to ignore asserts, i.e run with "-O" flag
- and obviously review the whole thing again

Note on dev-requirements:
- When using the normal script (`code/verifier.py`) with a verbose flag, graphviz visualizations of the computation graphs can be produced when the following additional packages are installed:
    - torchviz
- The generator (`generator/generate.py`) requires the following additional packages:
    - matplotlib
    - adversarial-robustness-toolbox
- The hyperparameter tuner (branch `tuner`) requires the following additional packages:
    - TODO
