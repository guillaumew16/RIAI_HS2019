\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{url} %, csquotes}
\usepackage[bookmarks]{hyperref}
%\usepackage{breqn}
\newcommand{\code}[1]{\texttt{#1}}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\eps}{\varepsilon}
\newcommand{\ind}{\mathbbm{1}}

% \theoremstyle{definition}
% \newtheorem{definition}{Definition}
% \newtheorem{notation}{Notation}
% \newtheorem{theorem}{Theorem}
% \newtheorem{proposition}{Proposition}
% \theoremstyle{remark}
% \newtheorem{remark}{Remark}
% \usepackage{graphicx}
% \usepackage{caption}
% \usepackage{subcaption}
% \usepackage{ulem}
% \usepackage[shortlabels]{enumitem}
% \setlist[itemize]{label=\textbullet}

\title{Title}
\author{Simone \textsc{Barbarbo}, Guillaume \textsc{Wang}}
% \date{}

\begin{document}
% \maketitle

\section{Notation}

\subsection{Mathematical formulation}

Consider a linear layer $\RR^N \to \RR^M$ (where $N \in \NN$ for FC nets and $N \in \NN_x \times \NN_y \times \NN_{channel}$ for conv nets).

The pair $(a_0, A), a_0 \in \RR^N, A \in \RR^{N \times k}$ represents the input zonotope $Z = \{ a_0 + A \cdot \eps, \eps \in [-1, 1]^k \}$. We used the notation $A \cdot \eps = (A_n \cdot \eps)_{n \in [N]}$, where $A_n \in \RR^k$ and $A_n \cdot \eps \in \RR$ is the regular dot-product.

Note that $k$ is the dimension of the zonotope, i.e the number of error terms. We will typically index over $n \in [N]$ and $l \in \{1,..., k\}$. 
$n \in [N]$ means $n \in \{1,...,N\}$ if $N \in \NN$, or $(n_x, n_y, n_c) \in [|N_x|] \times [|N_y|] \times [|N_c|]$ for conv nets.

\subsection{PyTorch tensor notation}

The mathematical objects described above are written resp. as:
\begin{itemize}
    \item $Z = (a_0, A)$: \code{Zonotope(a0, A)}
    \item $\RR^N$: tensors of shape \code{s} ($\simeq N$)
    \item $a_0 \in \RR^N$: a Tensor \code{a0} of shape \code{s}
    \item $A \in \RR^{N*k}$: a Tensor \code{A} of shape \code{(k, *s)} (we placed \code{k} first to pass through \code{torch.nn.Conv2d} layers easily, cf section \ref{sec:conv_zonotope_transfo})
    \item $\eps \in [-1, 1]^k$ is never written explicitly in the code, since we work with zonotopes
    \item $l \in \{1,...,k\}$: obviously we index \code{0<=l<k} instead. But in general it shouldn't be necessary, as everything can be written as vector operations
    \item $A_n \in \RR^N$: \code{A[n, :]}, where \code{n} is a tuple of length \code{len(s)} (i.e 1 for FC nets and 3 for CN nets). But again, it shouldn't be necessary, writing vector operations should suffice.
\end{itemize}

Tensors used in Conv2d are of shape $(N, C_{in}, H, W)$ and $(N, C_{out}, H_{out}, W_{out})$ where
\begin{itemize}
    \item $N$: batch size (in our case, it's the number of error terms)
    \item $C$: number of channels
    \item $H, W$: dimensions of the layer
\end{itemize}

\section{Convolutional zonotope transformation} \label{sec:conv_zonotope_transfo}

We can compute the zonotope approximation of the convolutional layers by considering each $\eps$ separately and computing the convolution of the coefficients corresponding to that $\eps$.

Let $I$ be the unknown input tensor of the convolutional layer, $K$ be the filter and $A$ be $I$'s coefficients in the zonotope approximation.
$K$ refers to the filter for a single channel. Neurons for a given layer are indexed by $[x,y]$.

\begin{equation*}
(I \times K) [x, y] = \sum_{i=0}^m \sum_{j=0}^m I[x+i - m/2, y+j - m/2] * K[i, j]
\end{equation*}

But $I[x, y] = \sum_{k=1}^n A_k[x, y]*\eps_k + A_0[x, y]$. So:

\begin{equation*}
(I \times K) [x, y] = \sum_{i=0}^m \sum_{j=0}^m (\sum_{k=1}^n A_k[x+i - m/2, y+j - m/2]*\eps_k + A_0[x+i - m/2, y+j - m/2]) * K[i, j]
\end{equation*}
Now I distribute the multiplication with the filter into the sum and change the order of the summations to get:
\begin{equation*}
(I \times K) [x, y] = \sum_{k=1}^n \sum_{i=0}^m \sum_{j=0}^m A_k[x+i - m/2, y+j - m/2] * K[i, j]*\eps_k + A_0[x+i - m/2, y+j - m/2] * K[i, j]
\end{equation*}
And now I take out the $\eps$ from the inner summation to get:
\begin{equation*}
(I \times K) [x, y] = \sum_{k=1}^n (\sum_{i=0}^m \sum_{j=0}^m A_k[x+i - m/2, y+j - m/2] * K[i, j])*\eps_k + \sum_{i=0}^m \sum_{j=0}^m A_0[x+i - m/2, y+j - m/2] * K[i, j]
\end{equation*}
As we can see from the equation, we have a zonotype where the center is given by a convolution over the centers of the input zonotope, $A_0$. While the coefficients of each $\eps$ are just a convolution over the coefficients of that $\eps$ in the input:
\begin{equation*}
(I \times K) [x, y] = \sum_{k=1}^n (A_k \times K)[x, y]*\eps_k + (A_0 \times K) [x, y]
\end{equation*}

It suffices to compute the $k+1$ convolutions of A and K, which can be done efficiently using pytorch.



\section{Loss function}

Output layer: $[o_1, o_2, ... o_n]$

Zonotope approximation of verification objective (target t):
\begin{equation}
Z = \sum_{i=1}^{n} \max(o_i - o_t)
\end{equation}

$Z > 0$ only if one or more of the $o_i$ is greater that $o_t$.

In particular if we compute the upper bound on $Z$:

\begin{equation}
L = max_\eps Z
\end{equation}

If $L = 0 => o_t >= o_i \forall i$ which is the property that we want to verify.

Else: $L > 0$ and we could minimize L by gradient descent with respect to lambdas.

In order to do that, we could build the entire Zonotope approximation with pythorch tensors and operators and then use it to compute gradients with respect to the lambdas.

\end{document}
